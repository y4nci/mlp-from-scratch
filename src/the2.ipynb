{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal, Optional, Union\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class gergen:\n",
    "    pass\n",
    "\n",
    "\n",
    "######################################################################\n",
    "#--------------------------- TYPE ALIASES ---------------------------#\n",
    "######################################################################\n",
    "\n",
    "DistributionType = Literal['uniform', 'normal']\n",
    "ActivationType = Literal['softmax', 'relu']\n",
    "\n",
    "MNISTSampleType = tuple[int, list[int]]\n",
    "\"\"\"\n",
    "The first element is the label of the image, and the second element is the pixel values of the image.\n",
    "\"\"\"\n",
    "MNISTDataType = list[MNISTSampleType]\n",
    "\n",
    "Atomic = Union[int, float]\n",
    "ListOrAtomic = Union[list, Atomic]\n",
    "GergenOrAtomic = Union['gergen', Atomic]\n",
    "\n",
    "isAtomic = lambda obj: isinstance(obj, (int, float))\n",
    "isList = lambda obj: isinstance(obj, list)\n",
    "isTuple = lambda obj: isinstance(obj, tuple)\n",
    "isGergen = lambda obj: isinstance(obj, gergen)\n",
    "isListOrAtomic = lambda obj: isList(obj) or isAtomic(obj)\n",
    "isGergenOrAtomic = lambda obj: isGergen(obj) or isAtomic(obj)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "#-------------------------- HELPER CLASSES --------------------------#\n",
    "######################################################################\n",
    "class TupleIndexedList:\n",
    "    def __init__(self, tuple_indexed_list: list):\n",
    "        self.tuple_indexed_list = tuple_indexed_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, int):\n",
    "            return self.tuple_indexed_list[index]\n",
    "        \n",
    "        if isTuple(index):\n",
    "            val_to_return = self.tuple_indexed_list\n",
    "\n",
    "            while len(index) > 0:\n",
    "                val_to_return = val_to_return[index[0]]\n",
    "                index = index[1:]\n",
    "\n",
    "            return val_to_return\n",
    "\n",
    "    def __setitem__(self, index, value):\n",
    "        if isinstance(index, int):\n",
    "            self.tuple_indexed_list[index] = value\n",
    "            return\n",
    "        \n",
    "        if isTuple(index):\n",
    "            val_to_return = self.tuple_indexed_list\n",
    "\n",
    "            while len(index) > 1:\n",
    "                val_to_return = val_to_return[index[0]]\n",
    "                index = index[1:]\n",
    "\n",
    "            val_to_return[index[0]] = value\n",
    "\n",
    "    def to_list(self):\n",
    "        return self.tuple_indexed_list\n",
    "    \n",
    "\n",
    "class Hyperparameters:\n",
    "    learning_rate: float\n",
    "\n",
    "    hidden_layer_size: int\n",
    "\n",
    "    epochs: int\n",
    "\n",
    "    def __init__(self, learning_rate: float, hidden_layer_size: int, epochs: int):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Learning Rate: {self.learning_rate}\\nHidden Layer Size: {self.hidden_layer_size}\\nEpochs: {self.epochs}'\n",
    "    \n",
    "\n",
    "class Result:\n",
    "    hyperparameters: Hyperparameters = None\n",
    "\n",
    "    average_losses_per_epoch: list[float]\n",
    "\n",
    "    def __init__(self, hyperparameters: Hyperparameters, average_losses_per_epoch):\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.average_losses_per_epoch = average_losses_per_epoch\n",
    "\n",
    "    def generate_plot(self):\n",
    "        plt.plot(self.average_losses_per_epoch)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Average Loss')\n",
    "        plt.title('Average Loss per Epoch for Hyperparameters\\n' + str(self.hyperparameters))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "######################################################################\n",
    "#------------------------- HELPER FUNCTIONS -------------------------#\n",
    "######################################################################\n",
    "def get_total_element_count_from_dimensions(boyut: tuple) -> int:\n",
    "    total_element_count = 1\n",
    "\n",
    "    for el in boyut:\n",
    "        total_element_count *= el\n",
    "\n",
    "    return total_element_count\n",
    "\n",
    "\n",
    "def get_total_element_count_from_nested_list(nested_list: list) -> int:\n",
    "    stringified_nested_list = str(nested_list)\n",
    "\n",
    "    return stringified_nested_list.count(',') + 1\n",
    "\n",
    "\n",
    "def create_nested_list(boyut: tuple, aralik_list: list, use_integer: bool, dagilim: DistributionType = 'uniform') -> list:\n",
    "    random_function = random.randint if use_integer else (random.uniform if dagilim == 'uniform' else random.gauss)\n",
    "\n",
    "    if len(boyut) == 0:\n",
    "        return random_function(*aralik_list)\n",
    "\n",
    "    total_length = 1\n",
    "\n",
    "    for el in boyut:\n",
    "        total_length *= el\n",
    "\n",
    "    unnested_list = [random_function(*aralik_list)\n",
    "        for _ in range(total_length)\n",
    "    ]\n",
    "    \n",
    "    return nest_list(unnested_list, boyut)\n",
    "\n",
    "\n",
    "def create_nested_list_with_fill(boyut: tuple, fill) -> list:\n",
    "    \"\"\"\n",
    "    I could've modified the function above to accept another parameter, but I didn't want to interfere with the random number generation\n",
    "    \"\"\"\n",
    "    if len(boyut) == 0:\n",
    "        return fill\n",
    "\n",
    "    total_length = 1\n",
    "\n",
    "    for el in boyut:\n",
    "        total_length *= el\n",
    "\n",
    "    unnested_list = [fill for _ in range(total_length)]\n",
    "    \n",
    "    return nest_list(unnested_list, boyut)\n",
    "\n",
    "\n",
    "def create_nested_list_using_xavier(boyut: tuple, dagilim: DistributionType) -> list:\n",
    "    aralik_list = []\n",
    "\n",
    "    if dagilim == 'uniform':\n",
    "        aralik_list = [\n",
    "            -math.sqrt(6 / (boyut[0] + boyut[1])),\n",
    "            math.sqrt(6 / (boyut[0] + boyut[1]))\n",
    "        ]\n",
    "    elif dagilim == 'normal':\n",
    "        aralik_list = [\n",
    "            0,\n",
    "            math.sqrt(2 / (boyut[0] + boyut[1]))\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError('Invalid distribution type')\n",
    "\n",
    "    return create_nested_list(boyut, aralik_list, False)\n",
    "\n",
    "\n",
    "def get_transpose_of_nested_list(nested_list: ListOrAtomic) -> ListOrAtomic:\n",
    "    if (\n",
    "        isAtomic(nested_list)\n",
    "    ):\n",
    "        return nested_list\n",
    "    \n",
    "    original_shape = get_dimensions_of_nested_list(nested_list)\n",
    "    new_shape = original_shape[::-1]\n",
    "\n",
    "    transposed_nested_list = create_nested_list_with_fill(new_shape, math.inf)\n",
    "\n",
    "    tuple_indexed_original = TupleIndexedList(nested_list)\n",
    "    tuple_indexed_transposed = TupleIndexedList(transposed_nested_list)\n",
    "\n",
    "    \"\"\"\n",
    "    this digital clock variable is used to keep track of the indexes. it is a list of 0s with the same length as the original shape, but\n",
    "    every digit at index i is reset to 0 when the index i is equal to the corresponding dimension of the original shape. this is used to\n",
    "    keep track of the indexes when iterating over the original nested list.\n",
    "\n",
    "    let's see an example:\n",
    "\n",
    "    if the original dimensions are (4, 3, 2, 5), the digital clock will start with value [0, 0, 0, 0]. at every iteration of the loop below\n",
    "    (which is used to iterate over the original nested list), the digital clock will be incremented by 1. when the first digit of the \n",
    "    digital clock is equal to 4, it is reset to 0 and the second digit is incremented by 1. this process continues until the total number\n",
    "    of iterations is equal to the total number of elements in the original nested list.\n",
    "    \"\"\"\n",
    "    digital_clock = [0 for _ in range(len(original_shape))]\n",
    "\n",
    "    element_count = get_total_element_count_from_dimensions(original_shape)\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "        tuple_indexed_transposed[tuple(digital_clock)[::-1]] = tuple_indexed_original[tuple(digital_clock)]\n",
    "\n",
    "        digital_clock[-1] += 1\n",
    "\n",
    "        for i in range(len(digital_clock) - 1, -1, -1):\n",
    "            if digital_clock[i] == original_shape[i]:\n",
    "                digital_clock[i] = 0\n",
    "                digital_clock[i - 1] += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "        if counter == element_count:\n",
    "            break\n",
    "\n",
    "    return tuple_indexed_transposed.to_list()\n",
    "\n",
    "\n",
    "def get_dimensions_of_nested_list(nested_list: list) -> tuple:\n",
    "    if isAtomic(nested_list):\n",
    "        return ()\n",
    "\n",
    "    boyut_list = []\n",
    "    current_nested_list = nested_list\n",
    "\n",
    "    while not isinstance(current_nested_list[0], int) and not isinstance(current_nested_list[0], float):\n",
    "        boyut_list.append(len(current_nested_list))\n",
    "        current_nested_list = current_nested_list[0]\n",
    "\n",
    "    boyut_list.append(len(current_nested_list))\n",
    "\n",
    "    return tuple(boyut_list)\n",
    "\n",
    "\n",
    "def unnest_list(nested_list: list) -> list:\n",
    "    if (\n",
    "        isAtomic(nested_list)\n",
    "    ):\n",
    "        return [nested_list]\n",
    "    \n",
    "    return [\n",
    "        el for sublist in nested_list for el in unnest_list(sublist)\n",
    "    ]\n",
    "\n",
    "\n",
    "def nest_list(unnested_list: list, boyut: tuple, total_element_count: int = -1) -> list:\n",
    "    if len(boyut) == 0:\n",
    "        return unnested_list[0]\n",
    "    \n",
    "    if len(boyut) == 1 and boyut[0] == len(unnested_list):\n",
    "        return unnested_list\n",
    "\n",
    "    if isAtomic(unnested_list):\n",
    "        return unnested_list\n",
    "\n",
    "    if total_element_count == -1:\n",
    "        total_element_count = 1\n",
    "\n",
    "        for el in boyut[1:]:\n",
    "            total_element_count *= el\n",
    "\n",
    "    sublist_element_count = int(total_element_count / boyut[1])\n",
    "\n",
    "    return [\n",
    "        nest_list(\n",
    "            unnested_list[i * total_element_count : (i + 1) * total_element_count], \n",
    "            boyut[1:], \n",
    "            sublist_element_count\n",
    "        ) for i in range(boyut[0])\n",
    "    ]\n",
    "\n",
    "\n",
    "def map_nested_list(nested_list: list, map_fn) -> list:\n",
    "    if (\n",
    "        isAtomic(nested_list)\n",
    "    ):\n",
    "        return map_fn(nested_list)\n",
    "    \n",
    "    return [\n",
    "        map_nested_list(el, map_fn) for el in nested_list\n",
    "    ]\n",
    "\n",
    "\n",
    "def diagonalize_gergen(gergen_instance: 'gergen') -> 'gergen':\n",
    "    if len(gergen_instance.boyut()) != 2:\n",
    "        raise ValueError('Gergen should be 2D')\n",
    "    \n",
    "    # gergen_instance is either a row vector [[x1, x2, x3 ... xn]] or a column vector [[x1], [x2], [x3] ... [xn]]\n",
    "\n",
    "    if gergen_instance.boyut()[0] == 1:\n",
    "        return gergen([\n",
    "            [gergen_instance.duzlestir()[i] if i == j else 0 for i in range(gergen_instance.boyut()[1])]\n",
    "                for j in range(gergen_instance.boyut()[1])\n",
    "        ])\n",
    "    \n",
    "    return gergen([\n",
    "        [gergen_instance.duzlestir()[i] if i == j else 0 for i in range(gergen_instance.boyut()[0])]\n",
    "            for j in range(gergen_instance.boyut()[0])\n",
    "    ])\n",
    "\n",
    "\n",
    "######################################################################\n",
    "#------------------------ OPERATION CLASSES -------------------------#\n",
    "######################################################################\n",
    "\n",
    "class Operation:\n",
    "    operands: Optional[list]\n",
    "\n",
    "    outputs: Optional[GergenOrAtomic]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.operands = None\n",
    "        self.outputs = None\n",
    "\n",
    "    def __call__(self, *operands):\n",
    "        \"\"\"\n",
    "        Makes an instance of the Operation class callable.\n",
    "        Stores operands and initializes outputs to None.\n",
    "        Invokes the forward pass of the operation with given operands.\n",
    "\n",
    "        ! RAISES AN ERROR IF self.operands AND self.outputs ARE NOT None.\n",
    "\n",
    "        Parameters:\n",
    "            *operands: Variable length operand list.\n",
    "\n",
    "        Returns:\n",
    "            The result of the forward pass of the operation.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.operands is not None or self.outputs is not None:\n",
    "            raise ValueError('Operation instance is already used')\n",
    "\n",
    "        self.operands = operands\n",
    "        self.outputs = None\n",
    "\n",
    "        self.outputs = self.ileri(*operands)\n",
    "\n",
    "        if isGergen(self.outputs):\n",
    "            self.outputs.set_operation(self)\n",
    "\n",
    "        return self.outputs\n",
    "    \n",
    "    def __str__(self):\n",
    "        str_to_return = '********** OPERATION **********\\n\\n'\n",
    "        str_to_return += f'Operation: {self.__class__.__name__}\\n\\n'\n",
    "        str_to_return += f'Operands: {self.operands}\\n\\n'\n",
    "        str_to_return += f'Outputs: {self.outputs}\\n\\n'\n",
    "        str_to_return += '*******************************\\n'\n",
    "\n",
    "        return str_to_return\n",
    "\n",
    "    def ileri(self, *operands):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the operation.\n",
    "        Must be implemented by subclasses to perform the actual operation.\n",
    "\n",
    "        Parameters:\n",
    "            *operands: Variable length operand list.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: If not overridden in a subclass.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def geri(self, grad_input):\n",
    "        \"\"\"\n",
    "        Defines the backward pass of the operation.\n",
    "        \n",
    "        Parameters:\n",
    "            output_gradient: Gradient of the loss wrt the output of this operation.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: If not overridden in a subclass.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def propagate_geri(self):\n",
    "        for operand in self.operands:\n",
    "            if (operand.get_operation() is not None): operand.get_operation().geri(operand.get_turev())\n",
    "\n",
    "\n",
    "class Addition(Operation):\n",
    "    def adder(self, left: ListOrAtomic, right: ListOrAtomic) -> ListOrAtomic:\n",
    "        \"\"\"\n",
    "        For gergen-to-gergen addition, it iterates over corresponding el- ements from both instances, adding them together. If one\n",
    "        operand is a scalar, this value is added to every element within the gergen instance. The method performs a dimensionality\n",
    "        check when both operands are gergen instances to ensure their shapes are compatible for element-wise operations. If the\n",
    "        dimensions do not align, a ValueError is raised, indicating a mismatch in dimensions. Additionally, if the other parameter is\n",
    "        of an unsupported type, a TypeError is raised to maintain type safety. The outcome of the addition is should be returned in a \n",
    "        new gergen object.\n",
    "        \"\"\"\n",
    "\n",
    "        if (\n",
    "            isList(left) and\n",
    "            isList(right)\n",
    "        ):\n",
    "            \"\"\"\n",
    "            both gergos (represented by lists)\n",
    "            \"\"\"\n",
    "            \n",
    "            return ([\n",
    "                self.adder(\n",
    "                    left[i],\n",
    "                    right[i]\n",
    "                ) for i in range(len(left))\n",
    "            ])\n",
    "        \n",
    "        if (\n",
    "            isAtomic(left) and\n",
    "            isAtomic(right)\n",
    "        ):\n",
    "            \"\"\"\n",
    "            both scalars\n",
    "            \"\"\"\n",
    "            return left + right\n",
    "        \n",
    "        if isAtomic(left):\n",
    "            \"\"\"\n",
    "            left is scalar\n",
    "            \"\"\"\n",
    "            return ([\n",
    "                self.adder(left, el) for el in right\n",
    "            ])\n",
    "\n",
    "        if isAtomic(right):\n",
    "            \"\"\"\n",
    "            right is scalar\n",
    "            \"\"\"\n",
    "            return ([\n",
    "                self.adder(el, right) for el in left\n",
    "            ])\n",
    "        \n",
    "        raise TypeError('Operands should be of type int, float, or gergen')\n",
    "\n",
    "    def ileri(self, *operands: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the addition operation.\n",
    "        Adds the given operands element-wise.\n",
    "\n",
    "        Parameters:\n",
    "            *operands: Variable length operand list.\n",
    "\n",
    "        Returns:\n",
    "            The result of the addition operation.\n",
    "        \"\"\"\n",
    "        if not all(isGergenOrAtomic(operand) for operand in operands):\n",
    "            raise TypeError('Operands should be of type int, float, or gergen')\n",
    "\n",
    "        if len(operands) < 2:\n",
    "            raise ValueError('Addition operation requires at least two operands')\n",
    "        \n",
    "        if len(operands) == 2:\n",
    "            if (\n",
    "                isGergen(operands[0]) and\n",
    "                isGergen(operands[1])\n",
    "            ):\n",
    "                if operands[0].boyut() != operands[1].boyut() and operands[0].boyut() != () and operands[1].boyut() != ():\n",
    "                    raise ValueError('Operands should have the same shape')\n",
    "\n",
    "            #! WE WILL NOT USE THE GERGEN OBJECT IN adder FUNCTION. INSTEAD, WE WILL PASS THE listeye() OF THE GERGEN OBJECT.\n",
    "            neutralised_operands = [\n",
    "                operand if isAtomic(operand) else operand.listeye()\n",
    "                    for operand in operands\n",
    "            ]\n",
    "\n",
    "            #! WE WILL RETURN THE RESULT AS A GERGEN OBJECT.\n",
    "            return gergen(self.adder(*neutralised_operands))\n",
    "\n",
    "        result = operands[0]\n",
    "\n",
    "        result = self(result, operands[1:])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def geri(self, grad_input):\n",
    "        \"\"\"\n",
    "        Defines the backward pass of the addition operation.\n",
    "        \n",
    "        Parameters:\n",
    "            grad_input: Gradient of the loss wrt the output of this operation.\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss with respect to the input of this operation.\n",
    "        \"\"\"\n",
    "\n",
    "        self.operands[0].set_turev(grad_input)\n",
    "        self.operands[1].set_turev(grad_input)\n",
    "\n",
    "        self.propagate_geri()\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class Subtraction(Operation):\n",
    "    def subtractor(self, left: ListOrAtomic, right: ListOrAtomic) -> ListOrAtomic:\n",
    "        \"\"\"\n",
    "        This method en- ables element-wise subtraction, either between two gergen instances or between a gergen and a scalar (int/float).\n",
    "        For gergen-to-gergen subtraction, corresponding elements from each instance are subtracted. When operating with a scalar, the\n",
    "        scalar value is subtracted from each element of the gergen instance. The method ensures that dimensions are compatible when both\n",
    "        operands are gergen instances, raising a ValueError if there is a mismatch. If the type of other is not supported, a TypeError is\n",
    "        raised. The outcome of the subtraction is should be returned in a new gergen ob- ject.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            isList(left) and\n",
    "            isList(right)\n",
    "        ):\n",
    "            \"\"\"\n",
    "            both gergos (represented by lists)\n",
    "            \"\"\"\n",
    "            \n",
    "            return ([\n",
    "                self.subtractor(\n",
    "                    left[i],\n",
    "                    right[i]\n",
    "                ) for i in range(len(left))\n",
    "            ])\n",
    "        \n",
    "        if (\n",
    "            isAtomic(left) and\n",
    "            isAtomic(right)\n",
    "        ):\n",
    "            \"\"\"\n",
    "            both scalars\n",
    "            \"\"\"\n",
    "            return left - right\n",
    "        \n",
    "        if isAtomic(left):\n",
    "            \"\"\"\n",
    "            left is scalar\n",
    "            \"\"\"\n",
    "            return ([\n",
    "                self.subtractor(left, el) for el in right\n",
    "            ])\n",
    "\n",
    "        if isAtomic(right):\n",
    "            \"\"\"\n",
    "            right is scalar\n",
    "            \"\"\"\n",
    "            return ([\n",
    "                self.subtractor(el, right) for el in left\n",
    "            ])\n",
    "        \n",
    "        raise TypeError('Operands should be of type int, float, or gergen')\n",
    "\n",
    "    def ileri(self, *operands: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the addition operation.\n",
    "        Adds the given operands element-wise.\n",
    "\n",
    "        Parameters:\n",
    "            *operands: Variable length operand list.\n",
    "\n",
    "        Returns:\n",
    "            The result of the addition operation.\n",
    "        \"\"\"\n",
    "        if not all(isGergenOrAtomic(operand) for operand in operands):\n",
    "            raise TypeError('Operands should be of type int, float, or gergen')\n",
    "\n",
    "        if len(operands) < 2:\n",
    "            raise ValueError('Addition operation requires at least two operands')\n",
    "        \n",
    "        if len(operands) == 2:\n",
    "            if (\n",
    "                isGergen(operands[0]) and\n",
    "                isGergen(operands[1])\n",
    "            ):\n",
    "                if operands[0].boyut() != operands[1].boyut() and operands[0].boyut() != () and operands[1].boyut() != ():\n",
    "                    raise ValueError('Operands should have the same shape')\n",
    "\n",
    "            #! WE WILL NOT USE THE GERGEN OBJECT IN adder FUNCTION. INSTEAD, WE WILL PASS THE listeye() OF THE GERGEN OBJECT.\n",
    "            neutralised_operands = [\n",
    "                operand if isAtomic(operand) else operand.listeye()\n",
    "                    for operand in operands\n",
    "            ]\n",
    "\n",
    "            #! WE WILL RETURN THE RESULT AS A GERGEN OBJECT.\n",
    "            return gergen(self.subtractor(*neutralised_operands))\n",
    "\n",
    "        result = operands[0]\n",
    "\n",
    "        result = self(result, operands[1:])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def geri(self, grad_input):\n",
    "        \"\"\"\n",
    "        Defines the backward pass of the addition operation.\n",
    "        \n",
    "        Parameters:\n",
    "            grad_input: Gradient of the loss wrt the output of this operation.\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss with respect to the input of this operation.\n",
    "        \"\"\"\n",
    "\n",
    "        self.operands[0].set_turev(grad_input)\n",
    "        self.operands[1].set_turev(-grad_input)\n",
    "\n",
    "        self.propagate_geri()\n",
    "\n",
    "        return grad_input\n",
    "    \n",
    "\n",
    "class Multiplication(Operation):\n",
    "    def multiplier(self, left, right):\n",
    "        \"\"\"\n",
    "        This method fa- cilitates the multiplication of the gergen either with another gergen instance for element-wise multiplication,\n",
    "        or with a scalar (int/float), yielding a new gergen ob- ject as the result. The other parameter is permitted to be a gergen, an\n",
    "        integer, or a floating-point number. Error handling is incorporated to manage cases where the other parameter is neither a gergen\n",
    "        object nor a numerical scalar. If the dimen- sions of two gergen instances do not align for element-wise multiplication, or if an\n",
    "        incompatible type is provided for other, a TypeError or ValueError is raised.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            isList(left) and\n",
    "            isList(right)\n",
    "        ):\n",
    "            \"\"\"\n",
    "            both gergos (represented by lists)\n",
    "            \"\"\"\n",
    "            \n",
    "            return ([\n",
    "                self.multiplier(\n",
    "                    left[i],\n",
    "                    right[i]\n",
    "                ) for i in range(len(left))\n",
    "            ])\n",
    "        \n",
    "        if (\n",
    "            isAtomic(left) and\n",
    "            isAtomic(right)\n",
    "        ):\n",
    "            \"\"\"\n",
    "            both scalars\n",
    "            \"\"\"\n",
    "            return left * right\n",
    "        \n",
    "        if isAtomic(left):\n",
    "            \"\"\"\n",
    "            left is scalar\n",
    "            \"\"\"\n",
    "            return ([\n",
    "                self.multiplier(left, el) for el in right\n",
    "            ])\n",
    "\n",
    "        if isAtomic(right):\n",
    "            \"\"\"\n",
    "            right is scalar\n",
    "            \"\"\"\n",
    "            return ([\n",
    "                self.multiplier(el, right) for el in left\n",
    "            ])\n",
    "        \n",
    "        raise TypeError('Operands should be of type int, float, or gergen')\n",
    "\n",
    "    def ileri(self, *operands: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the addition operation.\n",
    "        Adds the given operands element-wise.\n",
    "\n",
    "        Parameters:\n",
    "            *operands: Variable length operand list.\n",
    "\n",
    "        Returns:\n",
    "            The result of the addition operation.\n",
    "        \"\"\"\n",
    "        if not all(isGergenOrAtomic(operand) for operand in operands):\n",
    "            raise TypeError('Operands should be of type int, float, or gergen')\n",
    "\n",
    "        if len(operands) < 2:\n",
    "            raise ValueError('Multiplication operation requires at least two operands')\n",
    "        \n",
    "        if len(operands) == 2:\n",
    "            if (\n",
    "                isGergen(operands[0]) and\n",
    "                isGergen(operands[1])\n",
    "            ):\n",
    "                if operands[0].boyut() != operands[1].boyut() and operands[0].boyut() != () and operands[1].boyut() != ():\n",
    "                    raise ValueError('Operands should have the same shape')\n",
    "\n",
    "            #! WE WILL NOT USE THE GERGEN OBJECT IN adder FUNCTION. INSTEAD, WE WILL PASS THE listeye() OF THE GERGEN OBJECT.\n",
    "            neutralised_operands = [\n",
    "                operand if isAtomic(operand) else operand.listeye()\n",
    "                    for operand in operands\n",
    "            ]\n",
    "\n",
    "            #! WE WILL RETURN THE RESULT AS A GERGEN OBJECT.\n",
    "            return gergen(self.multiplier(*neutralised_operands))\n",
    "\n",
    "        result = operands[0]\n",
    "\n",
    "        result = self(result, operands[1:])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def geri(self, grad_input):\n",
    "        \"\"\"\n",
    "        Defines the backward pass of the addition operation.\n",
    "        \n",
    "        Parameters:\n",
    "            grad_input: Gradient of the loss wrt the output of this operation.\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss with respect to the input of this operation.\n",
    "        \"\"\"\n",
    "\n",
    "        self.operands[0].set_turev(grad_input * self.operands[1])\n",
    "        self.operands[1].set_turev(grad_input * self.operands[0])\n",
    "\n",
    "        self.propagate_geri()\n",
    "\n",
    "        return grad_input\n",
    "            \n",
    "\n",
    "class Division(Operation):\n",
    "    def divisor(self, left, right):\n",
    "        \"\"\"\n",
    "        This method implements division for the gergen, facilitating element-wise division by a scalar (an integer or a float), and\n",
    "        encapsulates the result in a new gergen instance. True divi- sion is employed, ensuring that the result is always a floating-point\n",
    "        number, consistent with Python 3.x division behavior, even if both operands are integers. Error handling mechanism ahould check\n",
    "        potential issues: if other is zero, a ZeroDivisionError is raised to prevent division by zero. Additionally, if other is not a\n",
    "        scalar type (int or float), a TypeError is raised to enforce the type requirement for the scalar divisor.\n",
    "        \"\"\"\n",
    "        if right == 0:\n",
    "            raise ZeroDivisionError('Division by zero is not allowed')\n",
    "\n",
    "        if (\n",
    "            isList(left) and\n",
    "            isList(right)\n",
    "        ):\n",
    "            \"\"\"\n",
    "            both gergos (represented by lists)\n",
    "            \"\"\"\n",
    "            \n",
    "            return ([\n",
    "                self.divisor(\n",
    "                    left[i],\n",
    "                    right[i]\n",
    "                ) for i in range(len(left))\n",
    "            ])\n",
    "        \n",
    "        if (\n",
    "            isAtomic(left) and\n",
    "            isAtomic(right)\n",
    "        ):\n",
    "            \"\"\"\n",
    "            both scalars\n",
    "            \"\"\"\n",
    "            return left / right\n",
    "        \n",
    "        if isAtomic(left):\n",
    "            \"\"\"\n",
    "            left is scalar\n",
    "            \"\"\"\n",
    "            return ([\n",
    "                self.divisor(left, el) for el in right\n",
    "            ])\n",
    "\n",
    "        if isAtomic(right):\n",
    "            \"\"\"\n",
    "            right is scalar\n",
    "            \"\"\"\n",
    "            return ([\n",
    "                self.divisor(el, right) for el in left\n",
    "            ])\n",
    "        \n",
    "        raise TypeError('Operands should be of type int, float, or gergen')\n",
    "\n",
    "    def ileri(self, *operands: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the addition operation.\n",
    "        Adds the given operands element-wise.\n",
    "\n",
    "        Parameters:\n",
    "            *operands: Variable length operand list.\n",
    "\n",
    "        Returns:\n",
    "            The result of the addition operation.\n",
    "        \"\"\"\n",
    "        if not all(isGergenOrAtomic(operand) for operand in operands):\n",
    "            raise TypeError('Operands should be of type int, float, or gergen')\n",
    "\n",
    "        if len(operands) < 2:\n",
    "            raise ValueError('Multiplication operation requires at least two operands')\n",
    "        \n",
    "        if len(operands) == 2:\n",
    "            if (\n",
    "                isGergen(operands[0]) and\n",
    "                isGergen(operands[1])\n",
    "            ):\n",
    "                if operands[0].boyut() != operands[1].boyut() and operands[0].boyut() != () and operands[1].boyut() != ():\n",
    "                    raise ValueError('Operands should have the same shape')\n",
    "\n",
    "            #! WE WILL NOT USE THE GERGEN OBJECT IN adder FUNCTION. INSTEAD, WE WILL PASS THE listeye() OF THE GERGEN OBJECT.\n",
    "            neutralised_operands = [\n",
    "                operand if isAtomic(operand) else operand.listeye()\n",
    "                    for operand in operands\n",
    "            ]\n",
    "\n",
    "            #! WE WILL RETURN THE RESULT AS A GERGEN OBJECT.\n",
    "            return gergen(self.divisor(*neutralised_operands))\n",
    "\n",
    "        result = operands[0]\n",
    "\n",
    "        result = self(result, operands[1:])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def geri(self, grad_input):\n",
    "        \"\"\"\n",
    "        Defines the backward pass of the addition operation.\n",
    "        \n",
    "        Parameters:\n",
    "            grad_input: Gradient of the loss wrt the output of this operation.\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss with respect to the input of this operation.\n",
    "        \"\"\"\n",
    "\n",
    "        self.operands[0].set_turev(grad_input / self.operands[1])\n",
    "        self.operands[1].set_turev(-grad_input * self.operands[0] / self.operands[1] ** 2)\n",
    "\n",
    "        self.propagate_geri()\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class ReLU(Operation):\n",
    "    def ileri(self, operand: gergen) -> 'gergen':\n",
    "        if (len(operand.boyut()) == 1):\n",
    "            return gergen([\n",
    "                max(0, el)\n",
    "                    for el in operand.listeye()\n",
    "            ])\n",
    "        \n",
    "        return gergen([\n",
    "            [max(0, el) for el in row]\n",
    "                for row in operand.listeye()\n",
    "        ])\n",
    "    \n",
    "    def geri(self, grad_input):\n",
    "        \"\"\"\n",
    "        Defines the backward pass of the ReLU operation.\n",
    "        \"\"\"\n",
    "        self.operands[0].set_turev(diagonalize_gergen(grad_input).ic_carpim(\n",
    "            gergen(\n",
    "                map_nested_list(self.operands[0].listeye(), lambda el: 1 if el > 0 else 0)\n",
    "            )\n",
    "        ))\n",
    "\n",
    "        self.propagate_geri()\n",
    "\n",
    "        return grad_input\n",
    "    \n",
    "\n",
    "class ForwardPass(Operation):\n",
    "    def ileri(self, *operands: gergen) -> gergen:\n",
    "        \"\"\"\n",
    "        The forward pass of a layer, which computes the output of the layer given the input x, weights W, and biases b.\n",
    "        \"\"\"\n",
    "        x, W, b = operands\n",
    "        \n",
    "        # element-wise add W.ic_carpim(x) and b\n",
    "\n",
    "        left = W.ic_carpim(x).listeye()\n",
    "        right = b.listeye()\n",
    "\n",
    "        # they are guaranteed to have the same shape and to be 2D\n",
    "\n",
    "        return gergen(\n",
    "            [\n",
    "                [ left[i][j] + right[i][j] for j in range(len(left[i])) ]\n",
    "                    for i in range(len(left))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def geri(self, grad_input):\n",
    "        \"\"\"\n",
    "        Defines the backward pass of the forward pass operation.\n",
    "        \"\"\"\n",
    "\n",
    "        x, W, b = self.operands\n",
    "\n",
    "        grad_x = W.devrik().ic_carpim(grad_input)\n",
    "        grad_W = grad_input.ic_carpim(x.devrik())\n",
    "        grad_b = grad_input\n",
    "\n",
    "        x.set_turev(grad_x)\n",
    "        W.set_turev(grad_W)\n",
    "        b.set_turev(grad_b)\n",
    "\n",
    "        self.propagate_geri()\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class Softmax(Operation):\n",
    "    def ileri(self, *operands: list[gergen]) -> 'gergen':\n",
    "        operand = operands[0]\n",
    "\n",
    "        try:\n",
    "            if (len(operand.boyut()) == 1):\n",
    "                return gergen([\n",
    "                    math.exp(el) / sum([math.exp(otherEl) for otherEl in operand.listeye()])\n",
    "                        for el in operand.listeye()\n",
    "                ])\n",
    "                    \n",
    "            return gergen([\n",
    "                [math.exp(el) / sum([math.exp(otherEl) for otherEl in operand.duzlestir()])\n",
    "                    for el in row\n",
    "                ] for row in operand.listeye()\n",
    "            ])\n",
    "        \n",
    "        except OverflowError:\n",
    "            print(operand)\n",
    "\n",
    "            exit(31)\n",
    "    \n",
    "    def geri(self, grad_input):\n",
    "        \"\"\"\n",
    "        Defines the backward pass of the softmax operation.\n",
    "        \"\"\"\n",
    "        self.operands[0].set_turev(grad_input)\n",
    "\n",
    "        self.propagate_geri()\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    \n",
    "class CrossEntropyLoss(Operation):\n",
    "    def ileri(self, *operands: list[gergen]) -> 'gergen':\n",
    "        y_pred, y_true = operands\n",
    "\n",
    "        y_pred_log = y_pred.log()\n",
    "\n",
    "        divider = - len(y_pred.listeye())\n",
    "\n",
    "        divided = gergen(y_pred_log.duzlestir()).ic_carpim(gergen(y_true.duzlestir())).listeye()\n",
    "\n",
    "        return gergen(\n",
    "            map_nested_list(divided, lambda el: el / divider)\n",
    "        )\n",
    "        \n",
    "    def geri(self, grad_input):\n",
    "        \"\"\"\n",
    "        Defines the backward pass of the cross entropy loss operation.\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred, y_true = self.operands\n",
    "\n",
    "        self.operands[0].set_turev((y_pred - y_true)) #.devrik())\n",
    "\n",
    "        self.propagate_geri()\n",
    "\n",
    "        return grad_input\n",
    "    \n",
    "\n",
    "######################################################################\n",
    "#------------------------- GERGOOOOOOOOOOOO -------------------------#\n",
    "######################################################################\n",
    "\n",
    "class gergen:\n",
    "\n",
    "    __veri = None #A nested list of numbers representing the data\n",
    "    D = None # Transpose of data\n",
    "    __boyut = None #Dimensions of the derivative (Shape)\n",
    "\n",
    "    __unnested_veri = None\n",
    "\n",
    "    __operation: Operation = None\n",
    "    \"\"\"\n",
    "    This attribute holds a reference to an Operation object. The operation rep- resents the computational operation that produces this\n",
    "    Gergen’s value when performing the forward pass. For example, this could be an addition, multiplication, or an activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    __turev = None\n",
    "    \"\"\"\n",
    "    This attribute holds the derivative (gradient) of the node concerning a particular variable. Initially, it is set to None, indicating\n",
    "    that the gradient has not been calculated yet. During the backward pass, this attribute should be modified and updated.\n",
    "    \"\"\"\n",
    "\n",
    "    __turev_gerekli: bool = True\n",
    "    \"\"\"\n",
    "    This boolean flag determines whether the gradient for this Gergen needs to be calculated. If the value of turev_gerekli is True, the\n",
    "    system will compute the gradients for this Gergen during the backward pass. If the user does not specify a value for turev_gerekli, it\n",
    "    should be set to True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, veri=None):\n",
    "    # The constructor for the 'gergen' class.\n",
    "    #\n",
    "    # This method initializes a new instance of a gergen object. The gergen can be\n",
    "    # initialized with data if provided; otherwise, it defaults to None, representing\n",
    "    # an empty tensor.\n",
    "    #\n",
    "    # Parameters:\n",
    "    # veri (int/float, list, list of lists, optional): A nested list of numbers that represents the\n",
    "    # gergen data. The outer list contains rows, and each inner list contains the\n",
    "    # elements of each row. If 'veri' is None, the tensor is initialized without data.\n",
    "    #\n",
    "    # Example:\n",
    "    # To create a tensor with data, pass a nested list:\n",
    "    # tensor = gergen([[1, 2, 3], [4, 5, 6]])\n",
    "    #\n",
    "    # To create an empty tensor, simply instantiate the class without arguments:\n",
    "    # empty_tensor = gergen()\n",
    "        self.__veri = veri\n",
    "\n",
    "        self.__boyut = get_dimensions_of_nested_list(veri)\n",
    "\n",
    "    def __getitem__(self, index) -> gergen:\n",
    "    #Indexing for gergen objects\n",
    "        if self.__veri is None:\n",
    "            raise ValueError('Tensor is empty')\n",
    "        \n",
    "        tuple_indexed = TupleIndexedList(self.__veri)\n",
    "\n",
    "        return gergen(tuple_indexed[index])\n",
    "\n",
    "    def __str__(self):\n",
    "        #Generates a string representation\n",
    "        string_to_print = \"\"\n",
    "\n",
    "        if self.__veri is None:\n",
    "            string_to_print += \"Boş gergen\"\n",
    "\n",
    "        elif isAtomic(self.__veri):\n",
    "            # If the tensor is a scalar, we can directly return the string representation of the scalar.\n",
    "            string_to_print += \"0 boyutlu skaler gergen:\\n\" + str(self.__veri)\n",
    "\n",
    "        else:\n",
    "            # If the tensor is not a scalar, we can make use of __boyut variable to construct a string representation.\n",
    "            for dim in self.__boyut:\n",
    "                string_to_print += str(dim) + \"x\"\n",
    "\n",
    "            string_to_print = string_to_print[:-1]\n",
    "            string_to_print += \" boyutlu gergen:\\n\" + str(self.__veri)\n",
    "\n",
    "        return string_to_print + \"\\n\"\n",
    "\n",
    "\n",
    "    def __mul__(self, other: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Multiplication operation for gergen objects.\n",
    "        Called when a gergen object is multiplied by another, using the '*' operator.\n",
    "        Could be element-wise multiplication or scalar multiplication, depending on the context.\n",
    "        \"\"\"\n",
    "        operation_instance = Multiplication()\n",
    "\n",
    "        operation_result = operation_instance(self, other)\n",
    "\n",
    "        return operation_result\n",
    "\n",
    "    def __truediv__(self, other: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Division operation for gergen objects.\n",
    "        Called when a gergen object is divided by another, using the '/' operator.\n",
    "        The operation is element-wise.\n",
    "        \"\"\"\n",
    "        operation_instance = Division()\n",
    "\n",
    "        operation_result = operation_instance(self, other)\n",
    "\n",
    "        return operation_result\n",
    "\n",
    "\n",
    "    def __add__(self, other: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Defines the addition operation for gergen objects.\n",
    "        Called when a gergen object is added to another, using the '+' operator.\n",
    "        The operation is element-wise.\n",
    "        \"\"\"\n",
    "        operation_instance = Addition()\n",
    "\n",
    "        operation_result = operation_instance(self, other)\n",
    "\n",
    "        return operation_result\n",
    "\n",
    "    def __sub__(self, other: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Subtraction operation for gergen objects.\n",
    "        Called when a gergen object is subtracted from another, using the '-' operator.\n",
    "        The operation is element-wise.\n",
    "        \"\"\"\n",
    "        operation_instance = Subtraction()\n",
    "\n",
    "        operation_result = operation_instance(self, other)\n",
    "\n",
    "        return operation_result\n",
    "\n",
    "    def __radd__(self, other: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Defines the addition operation for gergen objects when the left operand is a scalar.\n",
    "        Called when a scalar is added to a gergen object, using the '+' operator.\n",
    "        The operation is element-wise.\n",
    "        \"\"\"\n",
    "        return self.__add__(other)\n",
    "    \n",
    "    def __rsub__(self, other: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Subtraction operation for gergen objects when the left operand is a scalar.\n",
    "        Called when a scalar is subtracted from a gergen object, using the '-' operator.\n",
    "        The operation is element-wise.\n",
    "        \"\"\"\n",
    "        return other.__sub__(self)\n",
    "    \n",
    "    def __rmul__(self, other: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Multiplication operation for gergen objects when the left operand is a scalar.\n",
    "        Called when a scalar is multiplied by a gergen object, using the '*' operator.\n",
    "        The operation is element-wise.\n",
    "        \"\"\"\n",
    "        return self.__mul__(other)\n",
    "    \n",
    "    def __rtruediv__(self, other: GergenOrAtomic) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Division operation for gergen objects when the left operand is a scalar.\n",
    "        Called when a scalar is divided by a gergen object, using the '/' operator.\n",
    "        The operation is element-wise.\n",
    "        \"\"\"\n",
    "        return other.__truediv__(self)\n",
    "    \n",
    "    def __neg__(self) -> 'gergen':\n",
    "        \"\"\"\n",
    "        Negation operation for gergen objects.\n",
    "        Called when the negation operator '-' is used on a gergen object.\n",
    "        The operation is element-wise.\n",
    "        \"\"\"\n",
    "        return self.set_veri(\n",
    "            map_nested_list(self.__veri, lambda x: -x)\n",
    "        )\n",
    "\n",
    "    def set_veri(self, veri: list):\n",
    "        self.__veri = veri\n",
    "        self.__unnested_veri = None\n",
    "\n",
    "    def uzunluk(self):\n",
    "    # Returns the total number of elements in the gergen\n",
    "        if isAtomic(self.__veri):\n",
    "            return 1\n",
    "        \n",
    "        return len(self.duzlestir())\n",
    "\n",
    "    def boyut(self):\n",
    "    # Returns the shape of the gergen\n",
    "        return self.__boyut\n",
    "\n",
    "    def devrik(self):\n",
    "    # Returns the transpose of gergen\n",
    "        if self.D is None:\n",
    "            self.D = gergen(get_transpose_of_nested_list(self.__veri))\n",
    "\n",
    "        return self.D\n",
    "\n",
    "    def sin(self):\n",
    "    #Calculates the sine of each element in the given `gergen`.\n",
    "        return gergen(map_nested_list(self.__veri, lambda x: math.sin(x)))\n",
    "\n",
    "    def cos(self):\n",
    "    #Calculates the cosine of each element in the given `gergen`.\n",
    "        return gergen(map_nested_list(self.__veri, lambda x: math.cos(x)))\n",
    "\n",
    "    def tan(self):\n",
    "    #Calculates the tangent of each element in the given `gergen`.\n",
    "        return gergen(map_nested_list(self.__veri, lambda x: math.tan(x)))\n",
    "\n",
    "    def us(self, n: int):\n",
    "    #Raises each element of the gergen object to the power 'n'. This is an element-wise operation.\n",
    "        return gergen(map_nested_list(self.__veri, lambda x: x ** n))\n",
    "\n",
    "    def log(self):\n",
    "    #Applies the logarithm function to each element of the gergen object, using the base 10.\n",
    "        return gergen(map_nested_list(self.__veri, lambda x: math.log10(x)))\n",
    "\n",
    "    def ln(self):\n",
    "    #Applies the natural logarithm function to each element of the gergen object.\n",
    "        return gergen(map_nested_list(self.__veri, lambda x: math.log(x)))\n",
    "\n",
    "    def L1(self):\n",
    "    # Calculates and returns the L1 norm\n",
    "        return self.Lp(1)\n",
    "\n",
    "    def L2(self):\n",
    "    # Calculates and returns the L2 norm\n",
    "        return self.Lp(2)\n",
    "\n",
    "    def Lp(self, p):\n",
    "    # Calculates and returns the Lp norm, where p should be positive integer\n",
    "        if p <= 0:\n",
    "            raise ValueError('p should be a positive integer')\n",
    "\n",
    "        unnested_list = self.duzlestir()\n",
    "\n",
    "        return sum([abs(el) ** p for el in unnested_list]) ** (1 / p)\n",
    "\n",
    "    def listeye(self):\n",
    "    #Converts the gergen object into a list or a nested list, depending on its dimensions.\n",
    "        return self.__veri\n",
    "\n",
    "    def duzlestir(self):\n",
    "    #Converts the gergen object's multi-dimensional structure into a 1D structure, effectively 'flattening' the object.\n",
    "        if self.__unnested_veri is None:\n",
    "            self.__unnested_veri = unnest_list(self.__veri)\n",
    "        \n",
    "        return self.__unnested_veri\n",
    "\n",
    "    def boyutlandir(self, yeni_boyut):\n",
    "    #Reshapes the gergen object to a new shape 'yeni_boyut', which is specified as a tuple.\n",
    "        if not isTuple(yeni_boyut):\n",
    "            raise ValueError('yeni_boyut should be a tuple')\n",
    "        \n",
    "        current_uzunluk = self.uzunluk()\n",
    "        yeni_uzunluk = 1\n",
    "\n",
    "        for dim in yeni_boyut:\n",
    "            yeni_uzunluk *= dim\n",
    "\n",
    "        if yeni_uzunluk != current_uzunluk:\n",
    "            raise ValueError('The new shape should have the same number of elements as the original shape')\n",
    "\n",
    "        unnested_list = self.duzlestir()\n",
    "\n",
    "        return gergen(nest_list(unnested_list, yeni_boyut))\n",
    "\n",
    "    def ic_carpim(self, other):\n",
    "    #Calculates the inner (dot) product of this gergen object with another.\n",
    "        \"\"\"\n",
    "        ! SOLUTION TAKEN FROM : https://colab.research.google.com/drive/1foQsG4VU4K5pVDuIGoWi0Z4hbPk7fBbF?usp=sharing&pli=1&authuser=2#scrollTo=744_ff3uynvW\n",
    "\n",
    "        ic_carpim is defined for 1D and 2D gergens. dis_carpim is only for 1D gergens. topla and ortalama applies for n-dimensional gergens\n",
    "        see: https://odtuclass2023s.metu.edu.tr/mod/forum/discuss.php?d=757\n",
    "\n",
    "        Executes the inner prod- uct operation between two gergen objects. This method adheres to the mathematical definition of the inner\n",
    "        product, which requires both operands to be of the same di- mension.\n",
    "            For 1-D Tensors: both tensors must have the same dimensionality.\n",
    "            Matrix Product Representation: In cases where the gergen objects are treated as column vectors, the inner product can be\n",
    "            expressed through the matrix product x . y = xT y,\n",
    "            where xT denotes the transpose of vector x.\n",
    "            \n",
    "        Error Handling:\n",
    "            If either operand is not a gergen object, an error is raised to maintain type consistency, ensuring that the operation is\n",
    "            performed between two gergen instances.\n",
    "            For 1-D tensors, if the lengths of the vectors do not match, an error is thrown, emphasizing the requirement for equal\n",
    "            dimensionality in the inner product computation.\n",
    "            In the case of 2-D tensors, if the number of columns in the first matrix does not equal the number of rows in the second, an\n",
    "            error is raised, reflecting the necessity for compatible dimensions in matrix multiplication.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isGergen(other):\n",
    "            raise TypeError('Operands should be of type gergen')\n",
    "        \n",
    "        def is_vector(v):\n",
    "            return len(v.boyut()) == 1\n",
    "\n",
    "        def is_matrix(m):\n",
    "            return len(m.boyut()) == 2\n",
    "\n",
    "        def vector_dot_product(v1, v2):\n",
    "            if len(v1) != len(v2):\n",
    "                raise ValueError(\"Vectors must have the same length for dot product.\")\n",
    "            return sum(x * y for x, y in zip(v1, v2))\n",
    "\n",
    "        def matrix_multiply(m1, m2):\n",
    "            if len(m1[0]) != len(m2):\n",
    "                raise ValueError(\n",
    "                    \"The number of columns in the first matrix must match the number of rows in the second matrix.\")\n",
    "            return [[sum(a * b for a, b in zip(row_a, col_b)) for col_b in zip(*m2)] for row_a in m1]\n",
    "\n",
    "        if len(self.boyut()) > 2 or len(other.boyut()) > 2:\n",
    "            raise ValueError(\"Operands must both be either 1-D vectors or 2-D matrices.\")\n",
    "        elif is_vector(self) and is_vector(other):\n",
    "            # Perform vector dot product\n",
    "            result = vector_dot_product(self.listeye(), other.listeye())\n",
    "        elif is_matrix(self) and is_matrix(other):\n",
    "            # Perform matrix multiplication\n",
    "            result = matrix_multiply(self.listeye(), other.listeye())\n",
    "        else:\n",
    "            print(self.boyut(), self.listeye())\n",
    "            print(other.boyut(), other.listeye())\n",
    "            raise ValueError(\"Operands must both be either 1-D vectors or 2-D matrices.\")\n",
    "\n",
    "        # Return result\n",
    "        return gergen(result)\n",
    "\n",
    "    def dis_carpim(self, other):\n",
    "    #Calculates the outer product of this gergen object with another.\n",
    "        \"\"\"\n",
    "        ic_carpim is defined for 1D and 2D gergens. dis_carpim is only for 1D gergens. topla and ortalama applies for n-dimensional gergens\n",
    "        see: https://odtuclass2023s.metu.edu.tr/mod/forum/discuss.php?d=757\n",
    "        \"\"\"\n",
    "            \n",
    "        if not isGergen(other):\n",
    "            raise TypeError('Operands should be of type gergen')\n",
    "\n",
    "        if len(self.__boyut) != 1 or len(other.__boyut) != 1:\n",
    "            raise ValueError('Operands should be 1D gergens')\n",
    "        \n",
    "        if self.uzunluk() != other.uzunluk():\n",
    "            raise ValueError('Operands should have the same shape')\n",
    "        \n",
    "        result = []\n",
    "\n",
    "        for i in range(self.uzunluk()):\n",
    "            for j in range(other.uzunluk()):\n",
    "                result.append(self.__veri[i] * other.__veri[j])\n",
    "\n",
    "        return gergen(nest_list(result, (self.uzunluk(), other.uzunluk())))\n",
    "\n",
    "\n",
    "    def topla(self, eksen=None):\n",
    "    #Sums up the elements of the gergen object, optionally along a specified axis 'eksen'.\n",
    "        \"\"\"\n",
    "        Adds up values in gergen. If eksen is None, all elements are added. If eksen is not None, you can see the examples below:\n",
    "            Column-wise Addition (eksen=0): Elements over the vertical axis are added and returned as a gergen with the same size as the\n",
    "            number of columns.\n",
    "            Row-wise Addition (eksen=1): Elements over the horizontal axis are added and returned as a gergen with the same size as the\n",
    "            number of rows.\n",
    "        Error Handling:\n",
    "            If the specified eksen is not an integer or None, a TypeError is raised to indicate that eksen must be an integer or None.\n",
    "            When an eksen is provided, the function verifies that it is within the valid range of the data’s dimensions. If eksen exceeds\n",
    "            the dimensions, a ValueError is raised indicating that the specified eksen is out of bounds.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        ic_carpim is defined for 1D and 2D gergens. dis_carpim is only for 1D gergens. topla and ortalama applies for n-dimensional gergens.\n",
    "        see: https://odtuclass2023s.metu.edu.tr/mod/forum/discuss.php?d=757\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        the eksen param is the same param as the axis param in numpy's sum function.\n",
    "        \"\"\"\n",
    "\n",
    "        unnested_veri = self.duzlestir()\n",
    "\n",
    "        if eksen is not None:\n",
    "            if not isinstance(eksen, int):\n",
    "                raise TypeError('eksen should be an integer or None')\n",
    "            \n",
    "            if eksen < 0 or eksen >= len(self.__boyut):\n",
    "                raise ValueError('eksen is out of bounds')\n",
    "\n",
    "            boyut_list = list(self.__boyut)\n",
    "            boyut_list.pop(eksen)    \n",
    "\n",
    "            resulting_gergen_veri = []\n",
    "            resulting_gergen_boyut = tuple(boyut_list)\n",
    "\n",
    "            \"\"\"\n",
    "            about the algorithm:\n",
    "                what the algorithm does is:\n",
    "                    - flatten the list.\n",
    "                    - we will sum N elements in each iteration, where N is self.__boyut[eksen].\n",
    "                    - the N elements we will sum is going to be every Mth element, where M is the product of all elements in the list\n",
    "                      self.__boyut[eksen+1:]. If eksen is the last element, then M is 1.\n",
    "            \"\"\"\n",
    "\n",
    "            N = self.__boyut[eksen]\n",
    "            M = 1\n",
    "\n",
    "            for el in self.__boyut[eksen+1:]:\n",
    "                M *= el\n",
    "\n",
    "            summed_elem_counter = 0\n",
    "            total_elem_counter = 0\n",
    "            current_elem_sum = 0\n",
    "            offset = 0\n",
    "            offset_check_coefficient = 1\n",
    "\n",
    "            while total_elem_counter <= self.uzunluk() / N:\n",
    "                if total_elem_counter >= M * offset_check_coefficient:\n",
    "                    offset += M * (N - 1)\n",
    "                    offset_check_coefficient += 1\n",
    "\n",
    "                for i in range(total_elem_counter + offset, len(unnested_veri), M):\n",
    "                    current_elem_sum += unnested_veri[i]\n",
    "\n",
    "                    if summed_elem_counter == N - 1:\n",
    "                        resulting_gergen_veri.append(current_elem_sum)\n",
    "                        current_elem_sum = 0\n",
    "                        summed_elem_counter = 0\n",
    "                        break\n",
    "\n",
    "                    summed_elem_counter += 1\n",
    "                \n",
    "                total_elem_counter += 1\n",
    "\n",
    "            return gergen(nest_list(resulting_gergen_veri, resulting_gergen_boyut))\n",
    "\n",
    "        return sum(unnested_veri)\n",
    "\n",
    "    def ortalama(self, eksen=None):\n",
    "    #Calculates the average of the elements of the gergen object, optionally along a specified axis 'eksen'.\n",
    "        \"\"\"\n",
    "        Computes average (mean) of ele- ments in a tensor, with the flexibility to compute this average across different axes of the tensor\n",
    "        based on the eksen parameter (similar to topla).\n",
    "        When no eksen parameter is specified, the method computes the overall average of all elements within the tensor, treating it as a\n",
    "        flattened array. This is akin to calculating the mean value of a set of numbers.\n",
    "        Error Handling:\n",
    "            If the specified eksen is not an integer or None, a TypeError is raised to indicate that eksen must be an integer or None.\n",
    "            When an eksen is provided, the function verifies that it is within the valid range of the data’s dimensions. If eksen exceeds\n",
    "            the dimensions, a ValueError is raised indicating that the specified eksen is out of bounds.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        ic_carpim is defined for 1D and 2D gergens. dis_carpim is only for 1D gergens. topla and ortalama applies for n-dimensional gergens.\n",
    "        see: https://odtuclass2023s.metu.edu.tr/mod/forum/discuss.php?d=757\n",
    "        \"\"\"\n",
    "\n",
    "        divisor = self.uzunluk() if eksen is None else self.__boyut[eksen]\n",
    "\n",
    "        return self.topla(eksen) / divisor\n",
    "\n",
    "    def map(self, func):\n",
    "    #Applies a function to each element of the gergen object.\n",
    "        \"\"\"\n",
    "        Applies the specified function to each element in the gergen object, returning a new gergen object with the transformed values.\n",
    "        The function should be a lambda function or a user-defined function that can be applied to each element of the gergen object.\n",
    "        \"\"\"\n",
    "        return gergen(map_nested_list(self.__veri, func)) \n",
    "    \n",
    "    def get_operation(self) -> Operation:\n",
    "        \"\"\"\n",
    "        The operation represents the computational operation that produces this Gergen’s value when performing the forward pass.\n",
    "        \"\"\"\n",
    "        return self.__operation\n",
    "    \n",
    "    def set_operation(self, operation: Operation) -> None:\n",
    "        \"\"\"\n",
    "        Sets the operation for this Gergen.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.__operation is not None:\n",
    "            raise ValueError('Operation is already set')\n",
    "\n",
    "        self.__operation = operation\n",
    "\n",
    "    def get_turev(self):\n",
    "        \"\"\"\n",
    "        Returns the derivative of the node concerning a particular variable.\n",
    "        \"\"\"\n",
    "        return self.__turev\n",
    "    \n",
    "    def set_turev(self, turev):\n",
    "        \"\"\"\n",
    "        Sets the derivative of the node concerning a particular variable.\n",
    "        \"\"\"\n",
    "        self.__turev = turev\n",
    "\n",
    "    def get_turev_gerekli(self) -> bool:\n",
    "        \"\"\"\n",
    "        Returns whether the gradient for this Gergen needs to be calculated.\n",
    "        \"\"\"\n",
    "        return self.__turev_gerekli\n",
    "    \n",
    "    def set_turev_gerekli(self, turev_gerekli: bool):\n",
    "        \"\"\"\n",
    "        Sets whether the gradient for this Gergen needs to be calculated.\n",
    "        \"\"\"\n",
    "        self.__turev_gerekli = turev_gerekli\n",
    "    \n",
    "    def turev_al(self):\n",
    "        \"\"\"\n",
    "        Calculates the derivative of the gergen object using the chain rule.\n",
    "        \"\"\"\n",
    "        if self.__operation is None:\n",
    "            return gergen(create_nested_list_with_fill(self.boyut(), 1))\n",
    "        \n",
    "        self.__operation.geri(1)\n",
    "\n",
    "        return self.__turev\n",
    "    \n",
    "    def reset_turev(self):\n",
    "        \"\"\"\n",
    "        Resets the derivative of the gergen object.\n",
    "        \"\"\"\n",
    "        self.__turev = None\n",
    "\n",
    "    def subtract_gradient(self, learning_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Subtracts the calculated gradient from the current gergen object.\n",
    "        \"\"\"\n",
    "        if self.__turev is None:\n",
    "            raise ValueError('Gradient is not calculated yet')\n",
    "\n",
    "        flat_self = self.duzlestir()\n",
    "        flat_gradient = self.__turev.duzlestir()\n",
    "        flat_subtracted = [\n",
    "            flat_self[i] - (flat_gradient[i] * learning_rate)\n",
    "                for i in range(len(flat_self))\n",
    "        ]\n",
    "\n",
    "        reshaped_subtracted = nest_list(flat_subtracted, self.boyut())\n",
    "\n",
    "        self.set_veri(\n",
    "            reshaped_subtracted\n",
    "        )\n",
    "\n",
    "        self.reset_turev()\n",
    "        \n",
    "    def reset_operations_recursively(self):\n",
    "        \"\"\"\n",
    "        Resets the operation and derivative of the gergen object.\n",
    "        \"\"\"\n",
    "        if self.__operation is not None:\n",
    "            for operand in self.__operation.operands:\n",
    "                operand.reset_operations_recursively()\n",
    "\n",
    "            self.__operation.operands = None\n",
    "\n",
    "        self.__operation = None\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "#-------------------- FUNDAMENTAL FUNCTIONALITIES -------------------#\n",
    "######################################################################\n",
    "def cekirdek(sayi: int) -> None:\n",
    "    \"\"\"\n",
    "    Sets the seed for random number generation to ensure reproducibility of results. Before generating random numbers (for instance,\n",
    "    when initializing tensors with random values), you can call this function to set the seed.\n",
    "    \"\"\"\n",
    "    random.seed(sayi)\n",
    "\n",
    "\n",
    "def rastgele_dogal(boyut: tuple, aralik: tuple = (0, 100), dagilim: DistributionType = \"uniform\") -> gergen:\n",
    "    \"\"\"\n",
    "    Generates a gergen of specified dimensions with random integer values. The boyut parameter is a tuple specifying the dimensions of\n",
    "    the gergen to be generated. The aralik parameter is an optional tuple (min, max) specifying the range of random values, with a \n",
    "    default range of (0, 100). The dagilim parameter specifies the distribution of random values, with ‘uniform’ as the default\n",
    "    distribution. Possible values for dagilim include ‘uniform’ for a uniform distribution. You should raise ValueError if dagilim\n",
    "    parameter is given differently.\n",
    "    \"\"\"\n",
    "\n",
    "    if dagilim != 'uniform':\n",
    "        raise ValueError('dagilim parameter should be uniform')\n",
    "    \n",
    "    aralik_list: list = list(aralik)\n",
    "\n",
    "    # if boyut is 0, then we should return a scalar.\n",
    "    if len(boyut) == 0:\n",
    "        random_scalar = random.randint(*aralik_list)\n",
    "\n",
    "        return gergen(random_scalar)\n",
    "\n",
    "    return gergen(create_nested_list(boyut, aralik_list, True))\n",
    "\n",
    "\n",
    "def rastgele_gercek(boyut: tuple, aralik: tuple = (0.0,1.0), dagilim: DistributionType = None) -> gergen:\n",
    "    \"\"\"\n",
    "    Generates a gergen of specified dimensions with random floating-point values. The boyut parameter is a tuple specifying the dimensions\n",
    "    of the gergen to be generated. The aralik parameter is an optional tuple (min, max) specifying the range of random values, with a\n",
    "    default range of (0.0,1.0). The dagilim parameter specifies the distribution of random values, with ‘uniform’ as the default\n",
    "    distribution. Possible values for dagilim include ‘uniform’ for a uniform distribution. You should raise ValueError if dagilim\n",
    "    parameter is given differently.\n",
    "    \"\"\"\n",
    "\n",
    "    if dagilim != 'uniform' and dagilim is not None:\n",
    "        raise ValueError('dagilim parameter should be uniform')\n",
    "    \n",
    "    aralik_list: list = list(aralik)\n",
    "\n",
    "    # if boyut is 0, then we should return a scalar.\n",
    "    if len(boyut) == 0:\n",
    "        random_scalar = random.uniform(*aralik_list)\n",
    "\n",
    "        return gergen(random_scalar)\n",
    "    \n",
    "    return gergen(create_nested_list(boyut, aralik_list, False))\n",
    "\n",
    "\n",
    "def rastgele_xavier(boyut: tuple, dagilim: DistributionType = \"normal\") -> gergen:\n",
    "    return gergen(\n",
    "        create_nested_list_using_xavier(\n",
    "            boyut,\n",
    "            dagilim\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class Katman:\n",
    "    _x: gergen = None\n",
    "\n",
    "    _W: gergen = None\n",
    "\n",
    "    _b: gergen = None\n",
    "\n",
    "    _outputs: gergen = None\n",
    "\n",
    "    _activation: str = None\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int, activation: Optional[ActivationType] = None):\n",
    "        \"\"\"\n",
    "        With the given input and output sizes, the layer’s weighs and biases are initialised with appropriate shapes using the rastgele_gercek() function.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._x = rastgele_gercek((input_size, 1))\n",
    "        self._W = rastgele_xavier((output_size, input_size), 'uniform')\n",
    "        self._b = rastgele_gercek((output_size, 1), (-0.1, 0.1))\n",
    "        self._activation = activation\n",
    "\n",
    "    def calculate_forward(self):\n",
    "        \"\"\"\n",
    "        The forward method computes and returns the output of the layer given the input x, weights W, and biases b.\n",
    "        \"\"\"\n",
    "\n",
    "        forward_passer = ForwardPass()\n",
    "        raw_forward_pass: gergen = forward_passer(self._x, self._W, self._b)\n",
    "\n",
    "        if self._activation == 'relu':\n",
    "            relu_activator = ReLU()\n",
    "            return relu_activator(raw_forward_pass)\n",
    "\n",
    "        elif self._activation == 'softmax':\n",
    "            softmax_activator = Softmax()\n",
    "            return softmax_activator(raw_forward_pass)\n",
    "\n",
    "        else:\n",
    "            return raw_forward_pass\n",
    "        \n",
    "    def get_output(self):\n",
    "        \"\"\"\n",
    "        The get_output method returns the output of the layer.\n",
    "        \"\"\"\n",
    "        return self._outputs\n",
    "\n",
    "    def set_input(self, x: gergen):\n",
    "        \"\"\"\n",
    "        The set_input method sets the input of the layer to the given input x.\n",
    "        \"\"\"\n",
    "        self._x = x\n",
    "\n",
    "    def set_weights(self, W: gergen):\n",
    "        \"\"\"\n",
    "        The set_weights method sets the weights and biases of the layer to the given weights W.\n",
    "        \"\"\"\n",
    "        self._W = W\n",
    "\n",
    "    def set_bias(self, b: gergen):\n",
    "        \"\"\"\n",
    "        The set_bias method sets the biases of the layer to the given biases b.\n",
    "        \"\"\"\n",
    "        self._b = b\n",
    "\n",
    "    def set_activation(self, activation: ActivationType):\n",
    "        \"\"\"\n",
    "        The set_activation method sets the activation function of the layer to the given activation.\n",
    "        \"\"\"\n",
    "        self._activation = activation\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        The get_weights method returns the weights of the layer.\n",
    "        \"\"\"\n",
    "        return self._W\n",
    "    \n",
    "    def get_bias(self):\n",
    "        \"\"\"\n",
    "        The get_bias method returns the biases of the layer.\n",
    "        \"\"\"\n",
    "        return self._b\n",
    "    \n",
    "    def get_activation(self):\n",
    "        \"\"\"\n",
    "        The get_activation method returns the activation function of the layer.\n",
    "        \"\"\"\n",
    "        return self._activation\n",
    "    \n",
    "    def get_input(self):\n",
    "        \"\"\"\n",
    "        The get_input method returns the input of the layer.\n",
    "        \"\"\"\n",
    "        return self._x\n",
    "    \n",
    "    def ileri(self, x: gergen = None):\n",
    "        \"\"\"\n",
    "        The forward method computes and returns the output of the layer given the input x, weights W, and biases b.\n",
    "        \"\"\"\n",
    "        self.set_input(x if x is not None else self._x)\n",
    "\n",
    "        self._outputs = self.calculate_forward()\n",
    "\n",
    "        return self.get_output()\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    _input_size: int = None\n",
    "\n",
    "    _hidden_size: int = None\n",
    "\n",
    "    _output_size: int = None\n",
    "\n",
    "    _predicted_y_values: gergen = None\n",
    "\n",
    "    _loss_operation: CrossEntropyLoss = None\n",
    "\n",
    "    _learning_rate: float = None\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        \"\"\"\n",
    "        The MLP is initialized with three parameters: input size, hidden size, and output size, which correspond to the dimensions of the input layer, hidden\n",
    "        layer, and output layer, respectively.\n",
    "        Within the init method of the class, two layers are instantiated using the Layer class:\n",
    "         - hidden layer: This is the hidden layer of the network that takes inputs from the input layer and applies a ReLU (Rectified Linear Unit)\n",
    "         activation function to each neuron’s output.\n",
    "         - output layer: This is the output layer of the network that takes inputs from the hidden layer. The activation function used here is softmax,\n",
    "         which is appropriate for multi-class classification tasks.\n",
    "        \"\"\"\n",
    "\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "\n",
    "        self.hidden_layer = Katman(input_size, hidden_size, 'relu')\n",
    "        self.output_layer = Katman(hidden_size, output_size, 'softmax')\n",
    "\n",
    "    def ileri(self, inputs: gergen):\n",
    "        \"\"\"\n",
    "        The forward method computes the output of the network given the input.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        The part below \"normalises\" the inputs. The normalisation in question is NOT an actual normalisation, but rather a transformation of the inputs.\n",
    "        The inputs are transformed to a 2D array, where each column corresponds to a single input sample. This is done to ensure that the matrix\n",
    "        multiplication in the forward pass is done correctly.\n",
    "        \"\"\"\n",
    "\n",
    "        normalised_inputs = inputs.boyutlandir((self._input_size, 1))\n",
    "\n",
    "        self.hidden_layer.set_input(normalised_inputs)\n",
    "        self.hidden_layer.ileri()\n",
    "\n",
    "        self.output_layer.set_input(self.hidden_layer.get_output())\n",
    "        self.output_layer.ileri()\n",
    "\n",
    "        self._predicted_y_values = self.output_layer.get_output()\n",
    "\n",
    "        return self._predicted_y_values\n",
    "    \n",
    "    def calculate_loss(self, correct_y_values: gergen) -> gergen:\n",
    "        \"\"\"\n",
    "        The calculate_loss method computes the loss of the network given the true y values, using cross-entropy loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._loss_operation = CrossEntropyLoss()\n",
    "\n",
    "        res =  self._loss_operation(self._predicted_y_values, correct_y_values)\n",
    "\n",
    "        return res\n",
    "    \n",
    "\n",
    "def egit(\n",
    "    mlp: MLP,\n",
    "    inputs: gergen,\n",
    "    targets: gergen,\n",
    "    epochs: int,\n",
    "    learning_rate: float\n",
    "):\n",
    "    loss_history: list[float] = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        total_loss: float = 0.0\n",
    "\n",
    "        for i in tqdm(range(inputs.boyut()[0])):\n",
    "            sample = inputs[i]\n",
    "            target_label = targets[i].listeye()\n",
    "\n",
    "            target_expected_list = [0] * mlp._output_size\n",
    "            target_expected_list[target_label] = 1\n",
    "\n",
    "            target_expected_gergen = gergen(target_expected_list).boyutlandir((mlp._output_size, 1))\n",
    "\n",
    "            mlp.ileri(sample)\n",
    "\n",
    "            loss = mlp.calculate_loss(target_expected_gergen)\n",
    "\n",
    "            total_loss += loss.listeye()\n",
    "\n",
    "            loss.turev_al()\n",
    "\n",
    "            mlp.hidden_layer.get_weights().subtract_gradient(learning_rate)\n",
    "            mlp.hidden_layer.get_bias().subtract_gradient(learning_rate)\n",
    "            mlp.output_layer.get_weights().subtract_gradient(learning_rate)\n",
    "            mlp.output_layer.get_bias().subtract_gradient(learning_rate)\n",
    "\n",
    "            loss.reset_operations_recursively()\n",
    "\n",
    "        loss_history.append(total_loss / inputs.boyut()[0])\n",
    "\n",
    "    return mlp, loss_history\n",
    "\n",
    "\n",
    "def test(mlp: MLP, inputs: gergen, targets: gergen) -> gergen:\n",
    "    total_loss = gergen(0.0)\n",
    "\n",
    "    for i in range(inputs.boyut()[0]):\n",
    "        sample = inputs[i]\n",
    "        target_label = targets[i].listeye()\n",
    "\n",
    "        target_expected_list = [0] * mlp._output_size\n",
    "        target_expected_list[target_label] = 1\n",
    "\n",
    "        target_expected_gergen = gergen(target_expected_list).boyutlandir((mlp._output_size, 1))\n",
    "\n",
    "        mlp.ileri(sample)\n",
    "\n",
    "        loss = mlp.calculate_loss(target_expected_gergen)\n",
    "        \n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss / inputs.boyut()[0]\n",
    "\n",
    "\n",
    "def data_preprocessing(data_file: str) -> tuple[gergen, gergen]:\n",
    "    \"\"\"\n",
    "    Loads the data from the given file and preprocesses it.\n",
    "    \"\"\"\n",
    "\n",
    "    data = load_data(data_file)\n",
    "\n",
    "    images, labels = prepare_data(data)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_data(data_file: str) -> MNISTDataType:\n",
    "    \"\"\"\n",
    "    Loads the test and training data from the files \"test_data.csv\" and \"train_data.csv\", respectively, that are located in the\n",
    "    parent directory of the current working directory. The data is loaded from the files and returned as a tuple of lists. The data\n",
    "    is taken from MNIST. The MNIST dataset is a image collection of handwritten digits. These images have been normalized to a fixed\n",
    "    size of 28x28 = 784 pixels and centered to ensure consistency.\n",
    "\n",
    "    Each row in the data files represents an image. The first column contains the label of the image (the digit it represents), and\n",
    "    the remaining columns contain the pixel values of the image. The pixel values are integers between 0 and 255.\n",
    "    \"\"\"\n",
    "\n",
    "    data: MNISTDataType = []\n",
    "\n",
    "    with open(data_file, 'r') as test_file:\n",
    "        csv_reader = csv.reader(test_file)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            label = int(row[0])\n",
    "            image = list(map(int, row[1:]))\n",
    "\n",
    "            sample = (label, image)\n",
    "\n",
    "            data.append(sample)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_data(data: MNISTDataType) -> tuple[gergen, gergen]:\n",
    "    \"\"\"\n",
    "    Prepares the data for training and testing by separating the labels and images and converting them into the appropriate format.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = [sample[0] for sample in data]\n",
    "    images = [list(map(lambda val: (val / 256),  sample[1])) for sample in data]\n",
    "\n",
    "    labels_gergen = gergen(labels)\n",
    "    images_gergen = gergen(images)\n",
    "\n",
    "    return images_gergen, labels_gergen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "train_images, train_labels = data_preprocessing(\"../train_data.csv\")\n",
    "\n",
    "all_hyperparameter_combinations = [\n",
    "    Hyperparameters(learning_rate, hidden_layer_size, 10) for learning_rate in (0.01, 0.001, 0.0001) for hidden_layer_size in (5, 10, 30)\n",
    "]\n",
    "\n",
    "for hyperparameters in all_hyperparameter_combinations:\n",
    "    mlp = MLP(784, hyperparameters.hidden_layer_size, 10)\n",
    "\n",
    "    _, average_losses_per_epoch = egit(\n",
    "        mlp,\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        epochs=hyperparameters.epochs,\n",
    "        learning_rate=hyperparameters.learning_rate\n",
    "    )\n",
    "\n",
    "    results: Result = Result(\n",
    "        hyperparameters,\n",
    "        average_losses_per_epoch\n",
    "    )\n",
    "\n",
    "    results.generate_plot()\n",
    "```\n",
    "\n",
    "Running the code snippet above will generate a plot for each hyperparameter combination. The x-axis is the epoch number, and the y-axis is the average loss for that epoch. This allows me to compare the performance of different hyperparameter combinations.\n",
    "\n",
    "I ran this code snippet on the test data and found that the best hyperparameters were a learning rate of 0.01 and a hidden layer size of 30. I will use these hyperparameters to plot the final loss curve. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x112fdd4e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.8/Frameworks/Python.framework/Versions/3.11/lib/python3.11/weakref.py\", line 369, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "\n",
      "KeyboardInterrupt: \n",
      "  2%|▏         | 412/20000 [00:10<09:58, 32.75it/s]"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train_data_path = \"../train_data.csv\"\n",
    "test_data_path = \"../test_data.csv\"\n",
    "data, labels = data_preprocessing(train_data_path)\n",
    "test_data, test_labels = data_preprocessing(test_data_path)\n",
    "# Initialize the MLP with input, hidden, and output layers\n",
    "input_size = 28*28\n",
    "hidden_size = 30\n",
    "output_size = 10\n",
    "mlp = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Train the MLP using your preferred training loop\n",
    "epochs = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "trained_mlp, loss_history = egit(mlp, data, labels, epochs, learning_rate)\n",
    "test_loss = test(mlp, test_data, test_labels)\n",
    "\n",
    "result = Result(\n",
    "    Hyperparameters(\n",
    "        learning_rate=learning_rate,\n",
    "        hidden_layer_size=hidden_size,\n",
    "        epochs=epochs\n",
    "    ),\n",
    "    loss_history,\n",
    ")\n",
    "\n",
    "result.generate_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-2, 1e-3, 1e-4,1e-5]\n",
    "hidden_layer_sizes = [5, 10, 30]\n",
    "\n",
    "# results is dictionary mapping tuples of the form\n",
    "# (learning_rate, hidden_layer_size) to tuples of the form\n",
    "# (training_loss, test_loss).\n",
    "results = {}\n",
    "best_loss = 10   # The lowest test loss that we have seen so far.\n",
    "best_model = None # The MLP object that achieved the lowest test loss.\n",
    "best_lr = None  # The learning rate for the best model\n",
    "best_hl = None  # The hidden layer size for the best model\n",
    "\n",
    "best_result: Result = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for hl in hidden_layer_sizes:\n",
    "        hyperparameters = Hyperparameters(\n",
    "            learning_rate=lr,\n",
    "            hidden_layer_size=hl,\n",
    "            epochs=10\n",
    "        )\n",
    "\n",
    "        mlp = MLP(input_size=input_size, hidden_size=hl, output_size=output_size)\n",
    "\n",
    "        trained_mlp, loss_history = egit(mlp, data, labels, hyperparameters.epochs, learning_rate)\n",
    "\n",
    "        test_loss = test(mlp, test_data, test_labels).listeye()\n",
    "\n",
    "        train_loss = sum(loss_history) / len(loss_history)\n",
    "\n",
    "        print(f\"learning rate={lr} and hidden layer size={hl} provided train_loss={train_loss:.3f} and test_loss={test_loss:.3f}\")\n",
    "\n",
    "        # Save the results\n",
    "        results[(lr,hl)] = (train_loss, test_loss)\n",
    "\n",
    "        if test_loss < best_loss:\n",
    "            best_lr = lr\n",
    "            best_hl = hl\n",
    "            best_loss = test_loss\n",
    "            best_model = mlp\n",
    "\n",
    "            best_result = Result(\n",
    "                hyperparameters,\n",
    "                loss_history\n",
    "            )\n",
    "\n",
    "print(f'\\nLowest test loss achieved: {best_loss} with params hl={best_hl} and lr={best_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result.generate_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the MLP architecture\n",
    "class MLP_torch(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: Implement the MLP architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP_torch, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def calculate_loss(self, outputs, targets):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss: torch.Tensor = criterion(outputs, targets)\n",
    "        return loss\n",
    "    \n",
    "    def reset_gradients(self):\n",
    "        self.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "TODO: Handle the data and labels for PyTorch\n",
    "'''\n",
    "def data_preprocessing_torch(data_file: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Loads the data from the given file and preprocesses it.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = load_data(data_file)\n",
    "\n",
    "    images, labels = prepare_data(data)\n",
    "\n",
    "    images_tensor = torch.tensor(images.listeye(), dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels.listeye(), dtype=torch.long)\n",
    "\n",
    "    return images_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Implement the training pipeline using PyTorch\n",
    "'''\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_torch(mlp: MLP_torch, inputs: torch.Tensor, targets: torch.Tensor, epochs: int, learning_rate: float):\n",
    "    optimizer = optim.SGD(mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Implement the training pipeline for each example\n",
    "        for i in range(inputs.shape[0]):\n",
    "            sample = inputs[i]\n",
    "            target_label = targets[i]\n",
    "\n",
    "            target_expected = torch.tensor([1 if i == target_label else 0 for i in range(10)], dtype=torch.float32)\n",
    "\n",
    "            # Forward pass - with mlp.forward\n",
    "            outputs = mlp.forward(sample)\n",
    "\n",
    "            # Calculate Loss - with criterion (CrossEntropyLoss)\n",
    "            loss = mlp.calculate_loss(outputs, target_expected)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backward pass - Compute gradients for example\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters (Use an optimizer (e.g., SGD or Adam) to handle parameter updates)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Zero the gradients\n",
    "            mlp.zero_grad()\n",
    "\n",
    "        epoch_loss /= inputs.shape[0]\n",
    "\n",
    "        # Print epoch loss here if desired\n",
    "        print(f'EPOCH: {epoch}, Loss: {epoch_loss}')\n",
    "\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Implement the testing pipeline using PyTorch\n",
    "'''\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def test_torch(mlp: MLP_torch, inputs: torch.Tensor, targets: torch.Tensor):\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(inputs.shape[0]):\n",
    "        sample = inputs[i]\n",
    "        target_label = targets[i]\n",
    "\n",
    "        target_expected = torch.tensor([1 if i == target_label else 0 for i in range(10)], dtype=torch.float32)\n",
    "\n",
    "        # Forward pass - with mlp.forward\n",
    "        outputs = mlp.forward(sample)\n",
    "\n",
    "        # Calculate Loss - with criterion (CrossEntropyLoss)\n",
    "        loss = mlp.calculate_loss(outputs, target_expected)\n",
    "\n",
    "    loss /= inputs.shape[0]\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, Loss: 1.779928727608919\n",
      "EPOCH: 1, Loss: 1.639052625465393\n",
      "EPOCH: 2, Loss: 1.6256742791056633\n",
      "EPOCH: 3, Loss: 1.617601062220335\n",
      "EPOCH: 4, Loss: 1.6112832610189916\n",
      "EPOCH: 5, Loss: 1.6065807712256908\n",
      "EPOCH: 6, Loss: 1.6030305929660797\n",
      "EPOCH: 7, Loss: 1.5997474499702453\n",
      "EPOCH: 8, Loss: 1.5966894834220409\n",
      "EPOCH: 9, Loss: 1.5940262797772884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0001, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TODO: Train and test the PyTorch model\n",
    "'''\n",
    "\n",
    "# Load the data\n",
    "data, labels = data_preprocessing_torch(\"../train_data.csv\")\n",
    "test_data, test_labels = data_preprocessing_torch(\"../test_data.csv\")\n",
    "# Initialize the MLP with input, hidden, and output layers\n",
    "input_size = 28*28\n",
    "hidden_size = 30\n",
    "output_size = 10\n",
    "mlp = MLP_torch(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Train the MLP using PyTorch\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "trained_torch_mlp = train_torch(mlp, data, labels, epochs, learning_rate)\n",
    "test_torch(trained_torch_mlp,test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
